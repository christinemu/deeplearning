{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 参数管理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- 依靠深度学习框架管理操作参数：\n",
    "\n",
    "    * 访问参数，用于调试、诊断和可视化\n",
    "    * 参数初始化\n",
    "    * 在不同模型组件间共享参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "考虑一个单隐藏层的神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:59:39.200121Z",
     "iopub.status.busy": "2022-12-07T16:59:39.199802Z",
     "iopub.status.idle": "2022-12-07T16:59:40.341995Z",
     "shell.execute_reply": "2022-12-07T16:59:40.341206Z"
    },
    "origin_pos": 2,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0379],\n",
       "        [0.0994]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 参数访问"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 5,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 当通过`Sequential`类定义模型时，可以通过索引来访问模型的任意层\n",
    "    - 这就像模型是一个列表一样，每层的参数都在其属性中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:59:40.345895Z",
     "iopub.status.busy": "2022-12-07T16:59:40.345358Z",
     "iopub.status.idle": "2022-12-07T16:59:40.351780Z",
     "shell.execute_reply": "2022-12-07T16:59:40.350856Z"
    },
    "origin_pos": 7,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检查第二个全连接层的参数\n",
      "OrderedDict([('weight', tensor([[ 0.2658, -0.2868, -0.0546,  0.2135,  0.1780,  0.0741, -0.0212, -0.0069]])), ('bias', tensor([-0.0393]))])\n"
     ]
    }
   ],
   "source": [
    "print(f'检查第二个全连接层的参数\\n{net[2].state_dict()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 这个全连接层包含两个参数，分别是该层的权重和偏置\n",
    "    - 参数名称允许唯一标识每个参数，即使在包含数百个层的网络中也是如此"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- `state_dict()`是一个`Python`字典，将每层映射到对应的参数。一般下述对象拥有`state_dict`\n",
    "    - 包含可学习参数的层\n",
    "    - 优化器`torch.optim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 9,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 访问目标参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 每个参数都表示为参数类的一个实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:59:40.355017Z",
     "iopub.status.busy": "2022-12-07T16:59:40.354533Z",
     "iopub.status.idle": "2022-12-07T16:59:40.361115Z",
     "shell.execute_reply": "2022-12-07T16:59:40.360109Z"
    },
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "================\n",
      "Parameter containing:\n",
      "tensor([-0.0393], requires_grad=True)\n",
      "================\n",
      "tensor([-0.0393])\n"
     ]
    }
   ],
   "source": [
    "# 下面的代码从第二个全连接层（即第三个神经网络层）提取偏置，提取后返回的是一个参数类实例，并进一步访问该参数的值。\n",
    "print(type(net[2].bias))\n",
    "print('================')\n",
    "print(net[2].bias)\n",
    "print('================')\n",
    "print(net[2].bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 14,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "- 参数是复合的对象，包含值、梯度和额外信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:59:40.364515Z",
     "iopub.status.busy": "2022-12-07T16:59:40.364074Z",
     "iopub.status.idle": "2022-12-07T16:59:40.370363Z",
     "shell.execute_reply": "2022-12-07T16:59:40.369347Z"
    },
    "origin_pos": 16,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在上面这个网络中，由于还没有调用反向传播，所以参数的梯度处于初始状态\n",
    "net[2].weight.grad == None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 17,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 一次性访问所有参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 递归整个树提取每个子块的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:59:40.373816Z",
     "iopub.status.busy": "2022-12-07T16:59:40.373095Z",
     "iopub.status.idle": "2022-12-07T16:59:40.379087Z",
     "shell.execute_reply": "2022-12-07T16:59:40.378031Z"
    },
    "origin_pos": 19,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "=============================\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "print('=============================')\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- `named_parameters()`返回一个遍历模型块的参数的迭代器，包含参数的名称与参数本身"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 21,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 这提供了另一种访问网络参数的方式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:59:40.382676Z",
     "iopub.status.busy": "2022-12-07T16:59:40.381950Z",
     "iopub.status.idle": "2022-12-07T16:59:40.387675Z",
     "shell.execute_reply": "2022-12-07T16:59:40.386925Z"
    },
    "origin_pos": 23,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0393])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['2.bias'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 26,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 从嵌套块收集参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 观察在多个块相互嵌套中参数如何命名？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:59:40.391302Z",
     "iopub.status.busy": "2022-12-07T16:59:40.390503Z",
     "iopub.status.idle": "2022-12-07T16:59:40.400904Z",
     "shell.execute_reply": "2022-12-07T16:59:40.400156Z"
    },
    "origin_pos": 28,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0277],\n",
       "        [-0.0277]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义一个生成块的函数（可以说是“块工厂”），然后将这些块组合到更大的块中\n",
    "\n",
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                         nn.Linear(8, 4), nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        # 在这里嵌套\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:59:40.404269Z",
     "iopub.status.busy": "2022-12-07T16:59:40.403645Z",
     "iopub.status.idle": "2022-12-07T16:59:40.407896Z",
     "shell.execute_reply": "2022-12-07T16:59:40.407140Z"
    },
    "origin_pos": 33,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet) # 观察嵌套块结构和参数命名"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 35,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- 因为层是分层嵌套的，所以可以像通过嵌套列表索引一样访问它们"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:59:40.410923Z",
     "iopub.status.busy": "2022-12-07T16:59:40.410613Z",
     "iopub.status.idle": "2022-12-07T16:59:40.416456Z",
     "shell.execute_reply": "2022-12-07T16:59:40.415718Z"
    },
    "origin_pos": 37,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0941,  0.4905,  0.0289, -0.3920,  0.0739, -0.2282, -0.4643,  0.0487])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet[0][1][0].bias.data # 访问第一个主要的块中、第二个子块的第一层的偏置项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 40,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 参数初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 42,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "- 默认情况下，`PyTorch`会根据一个范围均匀地初始化权重和偏置矩阵\n",
    "- `PyTorch`的`nn.init`模块提供了多种预置初始化方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 45,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 内置初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:59:40.419626Z",
     "iopub.status.busy": "2022-12-07T16:59:40.419352Z",
     "iopub.status.idle": "2022-12-07T16:59:40.426766Z",
     "shell.execute_reply": "2022-12-07T16:59:40.426002Z"
    },
    "origin_pos": 47,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "权重为\n",
      "tensor([[ 0.0020, -0.0077, -0.0098,  0.0030],\n",
      "        [-0.0077,  0.0005,  0.0021, -0.0037],\n",
      "        [-0.0069, -0.0029,  0.0204, -0.0100],\n",
      "        [ 0.0166, -0.0042,  0.0133,  0.0012],\n",
      "        [-0.0089, -0.0059, -0.0025, -0.0075],\n",
      "        [-0.0233,  0.0114,  0.0035, -0.0037],\n",
      "        [ 0.0013,  0.0044,  0.0047,  0.0061],\n",
      "        [-0.0107,  0.0013,  0.0014,  0.0088]])\n",
      "偏置为\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# 下面的代码将所有权重参数初始化为标准差为0.01的高斯随机变量，且将偏置参数设置为0\n",
    "\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_normal)\n",
    "print(f'权重为\\n{net[0].weight.data}\\n偏置为\\n{net[0].bias.data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- ```python\n",
    "torch.nn.Module.apply(fn)\n",
    "```\n",
    "\n",
    "    - 将函数`fn`递归应用到**块的自身**和包含的**每个子块**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:59:40.430188Z",
     "iopub.status.busy": "2022-12-07T16:59:40.429543Z",
     "iopub.status.idle": "2022-12-07T16:59:40.436502Z",
     "shell.execute_reply": "2022-12-07T16:59:40.435761Z"
    },
    "origin_pos": 52,
    "slideshow": {
     "slide_type": "slide"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "权重为\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "偏置为\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# 还可以将所有参数初始化为给定的常数，比如初始化为1\n",
    "\n",
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_constant)\n",
    "print(f'权重为\\n{net[0].weight.data}\\n偏置为\\n{net[0].bias.data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 55,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- 对某些块应用不同的初始化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:59:40.439840Z",
     "iopub.status.busy": "2022-12-07T16:59:40.439218Z",
     "iopub.status.idle": "2022-12-07T16:59:40.445909Z",
     "shell.execute_reply": "2022-12-07T16:59:40.445156Z"
    },
    "origin_pos": 57,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "块0的权重为\n",
      "tensor([[-0.4273,  0.6390, -0.6410, -0.5123],\n",
      "        [ 0.6606, -0.5980, -0.2207, -0.5729],\n",
      "        [-0.4060, -0.0907, -0.0291, -0.0865],\n",
      "        [ 0.0791, -0.0904, -0.5871,  0.1705],\n",
      "        [ 0.2206,  0.3919, -0.5668,  0.5917],\n",
      "        [ 0.3217, -0.3232, -0.0541, -0.6284],\n",
      "        [-0.0609, -0.2158,  0.0890,  0.2609],\n",
      "        [ 0.6845,  0.4896,  0.4156,  0.6356]])\n",
      "块2的权重为\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "# 下面使用Xavier初始化方法初始化第一个神经网络层，然后将第三个神经网络层初始化为常量值42\n",
    "\n",
    "def init_xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "\n",
    "net[0].apply(init_xavier)\n",
    "net[2].apply(init_42)\n",
    "\n",
    "print(f'块0的权重为\\n{net[0].weight.data}')\n",
    "print(f'块2的权重为\\n{net[2].weight.data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 自定义初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 应对深度学习框架没有提供需要的初始化方法的情形"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 60,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- 下面的例子使用以下的分布为任意权重参数$w$定义初始化方法：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    w \\sim \\begin{cases}\n",
    "        U(5, 10) & \\text{ 可能性 } \\frac{1}{4} \\\\\n",
    "            0    & \\text{ 可能性 } \\frac{1}{2} \\\\\n",
    "        U(-10, -5) & \\text{ 可能性 } \\frac{1}{4}\n",
    "    \\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:59:40.449458Z",
     "iopub.status.busy": "2022-12-07T16:59:40.448848Z",
     "iopub.status.idle": "2022-12-07T16:59:40.457344Z",
     "shell.execute_reply": "2022-12-07T16:59:40.456599Z"
    },
    "origin_pos": 66,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([8, 4])\n",
      "Init weight torch.Size([1, 8])\n",
      "=================\n",
      "初始化后的权重为\n",
      "Parameter containing:\n",
      "tensor([[ 0.0000,  0.0000,  0.0000, -8.6178],\n",
      "        [ 0.0000, -7.1437, -7.6337,  9.6238],\n",
      "        [-0.0000,  0.0000, -0.0000,  8.8234],\n",
      "        [-8.9188, -0.0000,  5.5878,  0.0000],\n",
      "        [-7.3273, -0.0000, -7.6454, -7.2279],\n",
      "        [ 6.0692, -9.3481, -0.0000,  8.2564],\n",
      "        [-9.0458,  5.3331,  0.0000,  0.0000],\n",
      "        [ 6.2911, -0.0000,  8.2073, -6.7869]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 自定义一个`my_init`函数来应用到`net`\n",
    "\n",
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.shape) for name, param in m.named_parameters()][0])\n",
    "        nn.init.uniform_(m.weight, -10, 10)        # 用[-10,10]均匀分布初始化权重\n",
    "        m.weight.data *= m.weight.data.abs() >= 5  # 绝对值大于5保留，否则等于0\n",
    "\n",
    "net.apply(my_init)\n",
    "print('=================')\n",
    "print(f'初始化后的权重为\\n{net[0].weight}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 69,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- 注意，我们始终可以直接设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:59:40.460372Z",
     "iopub.status.busy": "2022-12-07T16:59:40.460090Z",
     "iopub.status.idle": "2022-12-07T16:59:40.466428Z",
     "shell.execute_reply": "2022-12-07T16:59:40.465690Z"
    },
    "origin_pos": 71,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.0000,  1.0000,  1.0000, -7.6178])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data[:] += 1\n",
    "net[0].weight.data[0, 0] = 42\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 75,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 参数绑定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 希望在多个层间共享参数：\n",
    "    - 可以定义一个稠密层，然后使用它的参数来设置另一个层的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:59:40.469511Z",
     "iopub.status.busy": "2022-12-07T16:59:40.469232Z",
     "iopub.status.idle": "2022-12-07T16:59:40.477075Z",
     "shell.execute_reply": "2022-12-07T16:59:40.476334Z"
    },
    "origin_pos": 77,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1884],\n",
       "        [0.1784]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 需要给共享层一个名称，以便可以引用它的参数\n",
    "\n",
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.Linear(8, 1))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# 检查参数是否相同\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "net[2].weight.data[0, 0] = 100\n",
    "# 确保它们实际上是同一个对象，而不只是有相同的值\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 81,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "- 上面例子表明第三个和第五个神经网络层的参数是绑定的，它们不仅值相等，而且由相同的张量表示。\n",
    "- 因此，如果我们改变其中一个参数，另一个参数也会改变"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "幻灯片",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
